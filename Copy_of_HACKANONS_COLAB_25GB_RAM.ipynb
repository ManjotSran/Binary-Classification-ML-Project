{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ManjotSran/Binary-Classification-ML-Project/blob/main/Copy_of_HACKANONS_COLAB_25GB_RAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cbgwZWWfWpp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAb77yZ9fzMG"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "filename = 'http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "df = pd.read_csv(filename, header=None)\n",
        "\n",
        "# Replace '?' with NaN and drop rows with missing values\n",
        "df.replace('?', np.nan, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encoding categorical features\n",
        "label_encoders = {}\n",
        "for column in df.columns:\n",
        "    if df[column].dtype == object:\n",
        "        label_encoders[column] = LabelEncoder()\n",
        "        df[column] = label_encoders[column].fit_transform(df[column])\n",
        "\n",
        "# Split the data into features and target label\n",
        "X = df.drop(14, axis=1)\n",
        "y = df[14]\n",
        "\n",
        "# Normalize the numerical features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Convert to numpy array for the model\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# Combine training features and target for the model\n",
        "training_data = np.column_stack((X_train, y_train))\n",
        "\n",
        "# Custom CART implementation\n",
        "# ... [Include your CART class and related functions here]\n",
        "def calculate_measure_of_goodness(y, left_y, right_y, num_classes):\n",
        "    P_L = len(left_y) / len(y)\n",
        "    P_R = 1 - P_L\n",
        "    goodness = 0\n",
        "\n",
        "    for j in range(num_classes):\n",
        "        P_j_tL = np.sum(left_y == j) / len(left_y) if len(left_y) > 0 else 0\n",
        "        P_j_tR = np.sum(right_y == j) / len(right_y) if len(right_y) > 0 else 0\n",
        "        goodness += abs(P_j_tL - P_j_tR)\n",
        "\n",
        "    return 2 * P_L * P_R * goodness\n",
        "\n",
        "def calculate_best_split(dataset, num_features, num_classes):\n",
        "    best_split = {}\n",
        "    max_goodness = -float(\"inf\")\n",
        "\n",
        "    for feature_index in range(num_features):\n",
        "        feature_values = np.unique(dataset[:, feature_index])\n",
        "        for value in feature_values:\n",
        "            left, right = split_dataset(dataset, feature_index, value)\n",
        "            if len(left) > 0 and len(right) > 0:\n",
        "                y, left_y, right_y = dataset[:, -1], left[:, -1], right[:, -1]\n",
        "                current_goodness = calculate_measure_of_goodness(y, left_y, right_y, num_classes)\n",
        "                if current_goodness > max_goodness:\n",
        "                    best_split[\"feature_index\"] = feature_index\n",
        "                    best_split[\"threshold\"] = value\n",
        "                    best_split[\"left\"] = left\n",
        "                    best_split[\"right\"] = right\n",
        "                    best_split[\"gain\"] = current_goodness\n",
        "                    max_goodness = current_goodness\n",
        "    return best_split\n",
        "\n",
        "def split_dataset(dataset, feature_index, threshold):\n",
        "    left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
        "    right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
        "    return left, right\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None, gain=None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "        self.gain = gain\n",
        "\n",
        "class CART:\n",
        "    def __init__(self, max_depth=10, min_size=2):\n",
        "        self.root = None\n",
        "        self.max_depth = max_depth\n",
        "        self.min_size = min_size\n",
        "\n",
        "    def build_tree(self, dataset, current_depth=0, num_classes=None):\n",
        "        X, y = dataset[:, :-1], dataset[:, -1]\n",
        "        num_features = X.shape[1]\n",
        "        if num_classes is None:\n",
        "            num_classes = len(np.unique(y))\n",
        "\n",
        "        # Stopping conditions\n",
        "        if len(set(y)) == 1 or current_depth >= self.max_depth:\n",
        "            return TreeNode(value=self.most_common_label(y))\n",
        "\n",
        "        # Calculate the best split\n",
        "        best_split = calculate_best_split(dataset, num_features, num_classes)\n",
        "        if best_split[\"gain\"] == 0 or len(best_split[\"left\"]) < self.min_size or len(best_split[\"right\"]) < self.min_size:\n",
        "            return TreeNode(value=self.most_common_label(y))\n",
        "\n",
        "        # Build left and right subtrees\n",
        "        left_subtree = self.build_tree(best_split[\"left\"], current_depth + 1, num_classes)\n",
        "        right_subtree = self.build_tree(best_split[\"right\"], current_depth + 1, num_classes)\n",
        "\n",
        "        # Create a tree node\n",
        "        return TreeNode(feature_index=best_split[\"feature_index\"], threshold=best_split[\"threshold\"],\n",
        "                        left=left_subtree, right=right_subtree, gain=best_split[\"gain\"])\n",
        "\n",
        "    def most_common_label(self, y):\n",
        "        return np.bincount(y.astype(int)).argmax()\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        num_classes = len(np.unique(dataset[:, -1]))\n",
        "        self.root = self.build_tree(dataset, num_classes=num_classes)\n",
        "\n",
        "    def predict(self, x, node=None):\n",
        "        if node is None:\n",
        "            node = self.root\n",
        "\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self.predict(x, node.left)\n",
        "        else:\n",
        "            return self.predict(x, node.right)\n",
        "\n",
        "    def predict_dataset(self, X):\n",
        "        return [self.predict(x) for x in X]\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "# CART Model Evaluation with Train-Test Split\n",
        "model = CART(max_depth=5, min_size=10)\n",
        "model.fit(training_data)\n",
        "predictions = model.predict_dataset(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_metric(y_test, predictions)\n",
        "print('Accuracy: %.3f%%' % accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pafL7Li0jyXW"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}